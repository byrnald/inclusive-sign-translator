{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection for ASL Gesture Recognition\n",
        "\n",
        "This notebook collects training data for ASL gesture recognition using Mediapipe hand detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Mediapipe\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "mp_drawing = mp.solutions.drawing_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "GESTURES = ['A', 'B', 'C', 'D', 'E']\n",
        "SAMPLES_PER_GESTURE = 100\n",
        "DATA_DIR = '../data/raw'\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Collecting {SAMPLES_PER_GESTURE} samples for each gesture: {GESTURES}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_landmarks(image):\n",
        "    \"\"\"Extract hand landmarks from image\"\"\"\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(rgb_image)\n",
        "    \n",
        "    if results.multi_hand_landmarks:\n",
        "        landmarks = []\n",
        "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
        "            landmarks.append([landmark.x, landmark.y, landmark.z])\n",
        "        return landmarks\n",
        "    return None\n",
        "\n",
        "def collect_gesture_data(gesture, num_samples):\n",
        "    \"\"\"Collect data for a specific gesture\"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    samples = []\n",
        "    \n",
        "    print(f\"\\nCollecting data for gesture: {gesture}\")\n",
        "    print(\"Press SPACE to capture sample, 'q' to quit\")\n",
        "    \n",
        "    while len(samples) < num_samples:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        # Detect hands\n",
        "        landmarks = extract_landmarks(frame)\n",
        "        \n",
        "        # Draw landmarks if detected\n",
        "        if landmarks:\n",
        "            # Draw landmarks on frame\n",
        "            annotated_frame = frame.copy()\n",
        "            mp_drawing.draw_landmarks(\n",
        "                annotated_frame,\n",
        "                results.multi_hand_landmarks[0],\n",
        "                mp_hands.HAND_CONNECTIONS\n",
        "            )\n",
        "            cv2.putText(annotated_frame, f\"Samples: {len(samples)}/{num_samples}\", \n",
        "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            cv2.putText(annotated_frame, f\"Gesture: {gesture}\", \n",
        "                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            cv2.imshow('Data Collection', annotated_frame)\n",
        "        else:\n",
        "            cv2.putText(frame, \"No hand detected\", (10, 30), \n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            cv2.imshow('Data Collection', frame)\n",
        "        \n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord(' '):  # Space to capture\n",
        "            if landmarks:\n",
        "                samples.append({\n",
        "                    'gesture': gesture,\n",
        "                    'landmarks': landmarks,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                })\n",
        "                print(f\"Captured sample {len(samples)}/{num_samples}\")\n",
        "        elif key == ord('q'):\n",
        "            break\n",
        "    \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    \n",
        "    # Save data\n",
        "    filename = f\"{DATA_DIR}/{gesture}_samples.json\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(samples, f, indent=2)\n",
        "    \n",
        "    print(f\"Saved {len(samples)} samples to {filename}\")\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data for all gestures\n",
        "all_data = {}\n",
        "\n",
        "for gesture in GESTURES:\n",
        "    samples = collect_gesture_data(gesture, SAMPLES_PER_GESTURE)\n",
        "    all_data[gesture] = samples\n",
        "\n",
        "print(\"\\nData collection completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all data\n",
        "combined_data = []\n",
        "for gesture, samples in all_data.items():\n",
        "    combined_data.extend(samples)\n",
        "\n",
        "# Save combined dataset\n",
        "with open(f\"{DATA_DIR}/combined_dataset.json\", 'w') as f:\n",
        "    json.dump(combined_data, f, indent=2)\n",
        "\n",
        "print(f\"Total samples collected: {len(combined_data)}\")\n",
        "print(f\"Samples per gesture: {len(combined_data) // len(GESTURES)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
