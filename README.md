# ğŸ¯ Inclusive Sign Translator

A real-time web application that detects basic sign language gestures through a webcam and translates them into text for beginners learning ASL.

## ğŸš€ Project Overview

This project helps people learn American Sign Language (ASL) by providing real-time gesture recognition and translation. The app uses computer vision and machine learning to detect hand gestures and display the corresponding letters or words.

## ğŸ“ Project Structure

```
inclusive-sign-translator/
â”œâ”€â”€ frontend/              # React web app for UI + camera feed
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ model/                 # Gesture recognition and ML model files
â”‚   â”œâ”€â”€ training_notebooks/
â”‚   â”œâ”€â”€ saved_models/
â”‚   â”œâ”€â”€ mediapipe_test.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ backend/ (optional)    # Flask backend for advanced processing
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ CONTRIBUTING.md
```

## ğŸ‘¥ Team Members & Roles

- **Member 1: Frontend/UI Developer**
  - React + TailwindCSS setup
  - Camera access and display
  - Real-time UI updates

- **Member 2: Model Developer**
  - Mediapipe hand detection
  - ML model training for ASL gestures
  - Model optimization and export

- **Member 3: Integration/Backend Developer**
  - Frontend-backend integration
  - Real-time gesture processing
  - Optional text-to-speech features

## ğŸ› ï¸ Tech Stack

- **Frontend:** React + Vite + TailwindCSS
- **Model:** Mediapipe Hands + TensorFlow.js
- **Backend:** Flask (Python) - Optional
- **AI Training:** Python Notebooks + Teachable Machine

## ğŸš€ Quick Start

### Frontend Setup
```bash
cd frontend
npm install
npm run dev
```

### Model Development
```bash
cd model
pip install -r requirements.txt
jupyter notebook
```

### Backend Setup (Optional)
```bash
cd backend
pip install -r requirements.txt
python app.py
```

## ğŸ§© Features (MVP)

- [x] Webcam access using getUserMedia
- [x] Real-time hand gesture detection
- [x] Classification of static signs (Aâ€“E minimum)
- [x] Dynamic text display
- [x] Responsive dark theme UI
- [ ] Voice output using Web Speech API

## ğŸ”„ Development Workflow

1. Create feature branches: `frontend`, `model`, `integration`
2. Work on your assigned components
3. Create pull requests for code review
4. Merge to `main` after approval

## ğŸ“ Git Commit Guidelines

- `feat: add camera feed and start detection button`
- `model: trained ASL Aâ€“E gestures`
- `fix: text overlay display on video stream`
- `docs: update setup instructions`

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed contribution guidelines.

## ğŸ“„ License

MIT License - see LICENSE file for details.

